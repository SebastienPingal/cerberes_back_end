name: AWS Deployment Pipeline

on:
  workflow_run:
    workflows: ["CI Pipeline"]
    branches: [master]
    types:
      - completed

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      # Debug step to check secrets are available (values will be masked)
      - name: Debug Secret Presence
        run: |
          if [ -n "${{ secrets.AWS_ACCESS_KEY_ID }}" ]; then 
            echo "AWS_ACCESS_KEY_ID is set âœ…"
          else
            echo "AWS_ACCESS_KEY_ID is NOT set âŒ"
          fi
          
          if [ -n "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]; then 
            echo "AWS_SECRET_ACCESS_KEY is set âœ…"
          else
            echo "AWS_SECRET_ACCESS_KEY is NOT set âŒ"  
          fi
          
          if [ -n "${{ secrets.AWS_REGION }}" ]; then 
            echo "AWS_REGION is set âœ…"
          else
            echo "AWS_REGION is NOT set âŒ"
          fi
          
          if [ -n "${{ secrets.JWT_SECRET }}" ]; then 
            echo "JWT_SECRET is set âœ…"
          else
            echo "JWT_SECRET is NOT set âŒ"
            echo "âš ï¸ JWT_SECRET is required for the application to function properly"
          fi
          
          if [ -n "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then 
            echo "EC2_SSH_PRIVATE_KEY is set âœ…"
            # Check if the key starts with the correct format
            if [[ "${{ secrets.EC2_SSH_PRIVATE_KEY }}" == "-----BEGIN"* ]]; then
              echo "SSH key appears to be in the correct format âœ…"
            else
              echo "SSH key does not start with '-----BEGIN' - may be incorrectly formatted âŒ"
            fi
          else
            echo "EC2_SSH_PRIVATE_KEY is NOT set âŒ"
          fi

      # Build the app directly instead of trying to download an artifact
      - name: Setup pnpm
        uses: pnpm/action-setup@v2
        with:
          version: 8.6.0

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 16
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install

      - name: Build application
        run: pnpm build

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Create terraform.tfvars
        run: |
          cat > terraform/terraform.tfvars << EOF
          aws_region      = "${{ secrets.AWS_REGION }}"
          app_name        = "cerberes"
          db_username     = "${{ secrets.DB_USERNAME }}"
          db_password     = "${{ secrets.DB_PASSWORD }}"
          ssh_key_name    = "${{ secrets.SSH_KEY_NAME }}"
          ec2_instance_type = "t2.micro"
          db_instance_class = "db.t3.micro"
          EOF
          
          echo "ğŸ”‘ Created terraform.tfvars with database credentials"

      # Check for existing resources and modify Terraform code accordingly
      - name: Check for existing resources
        id: check-resources
        run: |
          # Get the default VPC ID for security group checks
          DEFAULT_VPC_ID=$(aws ec2 describe-vpcs --filters "Name=isDefault,Values=true" --query "Vpcs[0].VpcId" --output text)
          echo "DEFAULT_VPC_ID=$DEFAULT_VPC_ID" >> $GITHUB_ENV
          
          # Check if we have existing VPCs
          VPC_COUNT=$(aws ec2 describe-vpcs --query "length(Vpcs)" --output text)
          
          if [ "$VPC_COUNT" -gt "0" ]; then
            echo "VPC_EXISTS=true" >> $GITHUB_ENV
            echo "âœ… VPCs exist - will use existing ones"
            # Modify the Terraform code to use existing VPCs
            sed -i '/data "aws_vpcs" "existing" {/,/count =/ s/count = 0/count = 1/g' terraform/main.tf
          else
            echo "VPC_EXISTS=false" >> $GITHUB_ENV
            echo "ğŸ†• No VPCs exist - will use default VPC"
          fi
          
          # Check if EC2 security group exists
          if aws ec2 describe-security-groups --filters "Name=group-name,Values=cerberes-ec2-sg" "Name=vpc-id,Values=$DEFAULT_VPC_ID" --query "SecurityGroups[0].GroupId" --output text &> /dev/null; then
            echo "EC2_SG_EXISTS=true" >> $GITHUB_ENV
            echo "âœ… EC2 Security Group exists - will use existing one"
            # Modify the Terraform code to use the existing security group
            sed -i '/data "aws_security_group" "ec2_existing" {/,/count =/ s/count = 0/count = 1/g' terraform/ec2.tf
          else
            echo "EC2_SG_EXISTS=false" >> $GITHUB_ENV
            echo "ğŸ†• EC2 Security Group does not exist - will create new one"
          fi
          
          # Check if DB security group exists
          if aws ec2 describe-security-groups --filters "Name=group-name,Values=cerberes-db-sg" "Name=vpc-id,Values=$DEFAULT_VPC_ID" --query "SecurityGroups[0].GroupId" --output text &> /dev/null; then
            echo "DB_SG_EXISTS=true" >> $GITHUB_ENV
            echo "âœ… DB Security Group exists - will use existing one"
            # Modify the Terraform code to use the existing security group
            sed -i '/data "aws_security_group" "db_existing" {/,/count =/ s/count = 0/count = 1/g' terraform/rds.tf
          else
            echo "DB_SG_EXISTS=false" >> $GITHUB_ENV
            echo "ğŸ†• DB Security Group does not exist - will create new one"
          fi
          
          # Check if EC2 instances with the tag Name=cerberes-instance exist
          EC2_COUNT=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=cerberes-instance" "Name=instance-state-name,Values=running,stopped" --query "length(Reservations[*].Instances[*])" --output text)
          
          if [ "$EC2_COUNT" -gt "0" ]; then
            echo "EC2_INSTANCE_EXISTS=true" >> $GITHUB_ENV
            EC2_ID=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=cerberes-instance" "Name=instance-state-name,Values=running,stopped" --query "Reservations[0].Instances[0].InstanceId" --output text)
            echo "âœ… EC2 Instance exists - will use existing one: $EC2_ID"
            # Modify the Terraform code to use the existing EC2 instance
            sed -i '/data "aws_instances" "existing" {/,/count =/ s/count = 0/count = 1/g' terraform/ec2.tf
          else
            echo "EC2_INSTANCE_EXISTS=false" >> $GITHUB_ENV
            echo "ğŸ†• EC2 Instance does not exist - will create new one"
          fi
          
          # Check if DB subnet group exists
          if aws rds describe-db-subnet-groups --db-subnet-group-name cerberes-db-subnet-group &> /dev/null; then
            echo "SUBNET_GROUP_EXISTS=true" >> $GITHUB_ENV
            echo "âœ… DB Subnet Group exists - will use existing one"
            # Modify the Terraform code to use the existing subnet group
            sed -i '/data "aws_db_subnet_group" "existing" {/,/count =/ s/count = 0/count = 1/g' terraform/rds.tf
          else
            echo "SUBNET_GROUP_EXISTS=false" >> $GITHUB_ENV
            echo "ğŸ†• DB Subnet Group does not exist - will create new one"
          fi
          
          # Check if DB instance exists
          if aws rds describe-db-instances --db-instance-identifier cerberes-db &> /dev/null; then
            echo "DB_INSTANCE_EXISTS=true" >> $GITHUB_ENV
            echo "âœ… DB Instance exists - will use existing one"
            # Modify the Terraform code to use the existing DB instance
            sed -i '/data "aws_db_instance" "existing" {/,/count =/ s/count = 0/count = 1/g' terraform/rds.tf
          else
            echo "DB_INSTANCE_EXISTS=false" >> $GITHUB_ENV
            echo "ğŸ†• DB Instance does not exist - will create new one"
          fi

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Terraform Plan
        working-directory: ./terraform
        run: terraform plan -out=tfplan

      - name: Terraform Apply
        id: terraform-apply
        working-directory: ./terraform
        run: terraform apply -auto-approve tfplan

      # Get EC2 public IP directly from AWS CLI instead of Terraform output
      - name: Get EC2 public IP
        if: steps.terraform-apply.outcome == 'success'
        id: ec2_ip
        run: |
          # Get the EC2 instance ID
          EC2_ID=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=cerberes-instance" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].InstanceId" --output text)
          
          # Get the public IP directly from AWS CLI
          IP_ADDRESS=$(aws ec2 describe-instances --instance-ids $EC2_ID --query "Reservations[0].Instances[0].PublicIpAddress" --output text)
          
          # Set the environment variable for use in subsequent steps
          echo "EC2_IP=$IP_ADDRESS" >> $GITHUB_ENV
          
          echo "ğŸ–¥ï¸ EC2 instance public IP: $IP_ADDRESS"

      # Get RDS endpoint directly from AWS CLI instead of Terraform output
      - name: Get RDS endpoint
        if: steps.terraform-apply.outcome == 'success'
        id: rds_endpoint
        run: |
          # Get the RDS endpoint directly from AWS CLI
          DB_ENDPOINT=$(aws rds describe-db-instances --db-instance-identifier cerberes-db --query "DBInstances[0].Endpoint.Address" --output text)
          
          # Set the environment variable for use in subsequent steps
          echo "DB_ENDPOINT=$DB_ENDPOINT" >> $GITHUB_ENV
          
          echo "ğŸ—„ï¸ RDS endpoint: $DB_ENDPOINT"

      # Add a delay to ensure EC2 instance is fully initialized
      - name: Wait for EC2 instance to initialize
        if: steps.terraform-apply.outcome == 'success'
        run: |
          echo "â³ Waiting 60 seconds for EC2 instance to fully initialize..."
          sleep 60

      # Create a temporary SSH key file
      - name: Setup SSH key
        if: steps.terraform-apply.outcome == 'success'
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/ec2_key
          chmod 600 ~/.ssh/ec2_key
          echo "ğŸ”‘ SSH key file created"
          
          # Test SSH connection
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/ec2_key ubuntu@${{ env.EC2_IP }} "echo SSH connection test successful"

      - name: Deploy application to EC2
        if: steps.terraform-apply.outcome == 'success'
        run: |
          # Create .env file for deployment
          cat > .env << EOF
          NODE_ENV=production
          DATABASE_URL=postgres://${{ secrets.DB_USERNAME }}:${{ secrets.DB_PASSWORD }}@${{ env.DB_ENDPOINT }}/cerberes
          JWT_SECRET=${{ secrets.JWT_SECRET }}
          PORT=3000
          EOF
          
          # Copy the node_modules directory to ensure all dependencies are included
          echo "ğŸ“¦ Preparing dependencies for deployment..."
          pnpm install --prod
          
          # Use scp directly instead of the action
          scp -o StrictHostKeyChecking=no -i ~/.ssh/ec2_key -r dist/ node_modules/ package.json pnpm-lock.yaml .env ubuntu@${{ env.EC2_IP }}:/home/ubuntu/app/
          echo "ğŸ“¦ Files deployed to EC2"

      - name: Configure and start application
        if: steps.terraform-apply.outcome == 'success'
        run: |
          # Use ssh directly instead of the action
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/ec2_key ubuntu@${{ env.EC2_IP }} << 'EOF'
            cd /home/ubuntu/app
            
            # Ensure required software is installed
            echo "ğŸ” Checking for required software..."
            if ! command -v nodejs &> /dev/null; then
              echo "ğŸ“¦ Installing Node.js and npm..."
              sudo apt update
              sudo apt install -y nodejs npm
            fi
            
            if ! command -v pm2 &> /dev/null; then
              echo "ğŸ“¦ Installing PM2 and pnpm..."
              sudo npm install -g pm2 pnpm
            fi
            
            # We're now skipping the pnpm install since we're copying node_modules directly
            echo "ğŸ“¦ Using pre-installed dependencies..."
            
            # Test the application directly first to check for errors
            echo "ğŸ” Testing application startup..."
            node -e "try { require('./dist/src/server'); console.log('âœ… Module loads successfully'); } catch(e) { console.error('âŒ Error loading module:', e); process.exit(1); }"
            
            # Stop the application if it's running
            echo "ğŸ”„ Restarting application..."
            pm2 stop cerberes || echo "ğŸ¤– App not running"
            
            # Start the application with more detailed logging
            echo "ğŸš€ Starting application..."
            pm2 start dist/src/server.js --name cerberes --log-date-format 'YYYY-MM-DD HH:mm:ss.SSS' --merge-logs
            pm2 save
            
            # Check if the application started successfully
            sleep 5
            if pm2 show cerberes | grep -q "status.*online"; then
              echo "âœ… Application started successfully!"
            else
              echo "âŒ Application failed to start. Checking logs..."
              pm2 logs cerberes --lines 50
              exit 1
            fi
            
            # Configure PM2 to start on boot
            echo "ğŸ”„ Setting up PM2 to start on boot..."
            pm2 startup | grep "sudo" | bash || echo "âš ï¸ PM2 startup command failed, but continuing..."
            pm2 save
            
            # Verify the application is running and listening on port 3000
            echo "ğŸ” Verifying application status..."
            pm2 list
            
            # Check if the application is listening on port 3000
            if netstat -tulpn 2>/dev/null | grep -q ":3000"; then
              echo "âœ… Application is listening on port 3000"
            else
              echo "âš ï¸ Application may not be listening on port 3000"
              sudo apt-get install -y net-tools
              netstat -tulpn | grep node
            fi
            
            echo "ğŸš€ Deployment completed"
          EOF
            
      - name: Cleanup on failure
        if: failure() && steps.terraform-apply.outcome == 'failure'
        working-directory: ./terraform
        run: |
          echo "ğŸ§¹ Cleaning up failed deployment..."
          terraform destroy -auto-approve
          echo "â™»ï¸ Resources cleaned up."
