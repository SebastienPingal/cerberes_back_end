name: AWS Deployment Pipeline

on:
  push:
    branches: [ master ]
  workflow_run:
    workflows: ["CI Pipeline"]
    branches: [master]
    types:
      - completed

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'push' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      # Debug step to check secrets are available (values will be masked)
      - name: Debug Secret Presence
        run: |
          if [ -n "${{ secrets.AWS_ACCESS_KEY_ID }}" ]; then 
            echo "AWS_ACCESS_KEY_ID is set ✅"
          else
            echo "AWS_ACCESS_KEY_ID is NOT set ❌"
          fi
          
          if [ -n "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]; then 
            echo "AWS_SECRET_ACCESS_KEY is set ✅"
          else
            echo "AWS_SECRET_ACCESS_KEY is NOT set ❌"  
          fi
          
          if [ -n "${{ secrets.AWS_REGION }}" ]; then 
            echo "AWS_REGION is set ✅"
          else
            echo "AWS_REGION is NOT set ❌"
          fi

      # Build the app directly instead of trying to download an artifact
      - name: Setup pnpm
        uses: pnpm/action-setup@v2
        with:
          version: 8.6.0

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 16
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install

      - name: Build application
        run: pnpm build

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Create terraform.tfvars
        run: |
          cat > terraform/terraform.tfvars << EOF
          aws_region      = "${{ secrets.AWS_REGION }}"
          app_name        = "cerberes"
          db_username     = "${{ secrets.DB_USERNAME }}"
          db_password     = "${{ secrets.DB_PASSWORD }}"
          ssh_key_name    = "${{ secrets.SSH_KEY_NAME }}"
          ec2_instance_type = "t2.micro"
          db_instance_class = "db.t3.micro"
          EOF

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Check for existing resources
        id: check-resources
        working-directory: ./terraform
        continue-on-error: true
        run: |
          # Check if DB subnet group exists
          if aws rds describe-db-subnet-groups --db-subnet-group-name cerberes-db-subnet-group &> /dev/null; then
            echo "DB_SUBNET_GROUP_EXISTS=true" >> $GITHUB_ENV
            echo "✅ DB Subnet Group exists - will handle in Terraform"
          else
            echo "DB_SUBNET_GROUP_EXISTS=false" >> $GITHUB_ENV
            echo "🆕 DB Subnet Group does not exist"
          fi
          
          # Check if DB instance exists
          if aws rds describe-db-instances --db-instance-identifier cerberes-db &> /dev/null; then
            echo "DB_INSTANCE_EXISTS=true" >> $GITHUB_ENV
            echo "✅ DB Instance exists - will handle in Terraform"
          else
            echo "DB_INSTANCE_EXISTS=false" >> $GITHUB_ENV
            echo "🆕 DB Instance does not exist"
          fi

      - name: Import existing resources
        working-directory: ./terraform
        if: env.DB_SUBNET_GROUP_EXISTS == 'true'
        continue-on-error: true
        run: |
          # Create Terraform state entry for existing DB subnet group
          echo "🔄 Importing existing DB subnet group into Terraform state..."
          terraform import aws_db_subnet_group.db_subnet_group cerberes-db-subnet-group

      - name: Terraform Plan
        working-directory: ./terraform
        run: terraform plan -out=tfplan

      - name: Terraform Apply
        id: terraform-apply
        working-directory: ./terraform
        run: terraform apply -auto-approve tfplan

      - name: Get EC2 public IP
        if: steps.terraform-apply.outcome == 'success'
        working-directory: ./terraform
        id: ec2_ip
        run: |
          echo "EC2_IP=$(terraform output -raw public_ip)" >> $GITHUB_ENV

      - name: Deploy application to EC2
        if: steps.terraform-apply.outcome == 'success' && env.EC2_IP != ''
        uses: appleboy/scp-action@master
        with:
          host: ${{ env.EC2_IP }}
          username: ec2-user
          key: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
          source: "dist/,package.json,pnpm-lock.yaml,.env"
          target: "/home/ec2-user/app"

      - name: Configure and start application
        if: steps.terraform-apply.outcome == 'success' && env.EC2_IP != ''
        uses: appleboy/ssh-action@master
        with:
          host: ${{ env.EC2_IP }}
          username: ec2-user
          key: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
          script: |
            cd /home/ec2-user/app
            
            # Add production environment variables
            cat > .env << EOF
            NODE_ENV=production
            DATABASE_URL=postgres://${{ secrets.DB_USERNAME }}:${{ secrets.DB_PASSWORD }}@$(terraform output -raw db_endpoint)/cerberes
            PORT=3000
            EOF
            
            # Install dependencies and start with PM2
            pnpm install --prod
            pm2 stop cerberes || echo "🤖 App not running"
            pm2 start dist/src/server.js --name cerberes
            pm2 save
            
            echo "🚀 Deployment completed"
            
      - name: Cleanup on failure
        if: failure() && steps.terraform-apply.outcome == 'failure'
        working-directory: ./terraform
        run: |
          echo "🧹 Cleaning up failed deployment..."
          terraform destroy -auto-approve
          echo "♻️ Resources cleaned up."
